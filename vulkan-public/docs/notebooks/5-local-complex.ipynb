{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from vulkan_public.beam.local.runner import PolicyRunner\n",
    "from vulkan_public.core.policy import Policy\n",
    "from vulkan_public.schemas import DataSourceSpec\n",
    "from vulkan_public.spec.dependency import INPUT_NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE_A_ID = \"api:data-source-a:v1\"\n",
    "MODEL_A_ID = \"api:model-a:v1\"\n",
    "DATA_SOURCE_B_ID = \"api:data-source-b:v23\"\n",
    "MODEL_B_ID = \"api:model-b-serasa-teste-etc-etc:v34\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "We'll create a Parquet file with our input data, and a second file to act as a \"Data Source\".\n",
    "\n",
    "Data Sources bring external data into your workflow. \n",
    "This can be done by consulting a bureau, or by having some test data, like in our case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_a_test_schema = {\n",
    "    \"name\": DATA_SOURCE_A_ID,\n",
    "    \"keys\": [\"tax_id\"],\n",
    "    \"source\": {\n",
    "        \"path\": \"data_source_a.parquet\",\n",
    "    },\n",
    "}\n",
    "\n",
    "model_a_test_schema = {\n",
    "    \"name\": MODEL_A_ID,\n",
    "    \"keys\": [\"tax_id\"],\n",
    "    \"source\": {\n",
    "        \"path\": \"data_source_model_a.parquet\",\n",
    "    },\n",
    "}\n",
    "\n",
    "data_sources = [\n",
    "    DataSourceSpec.model_validate(data_source_a_test_schema),\n",
    "    DataSourceSpec.model_validate(model_a_test_schema),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/simple_bkt_lg.csv\")\n",
    "df[\"month\"] = df[\"month\"].astype(str)\n",
    "df[\"tax_id\"] = df[\"tax_id\"].astype(str)\n",
    "\n",
    "df.to_parquet(\"input.parquet\")\n",
    "\n",
    "data_source_a = df[[\"tax_id\"]]\n",
    "data_source_a[\"valid\"] = np.random.choice(\n",
    "    [True, False], size=data_source_a.shape[0], p=[0.5, 0.5]\n",
    ")\n",
    "data_source_a.to_parquet(\"data_source_a.parquet\")\n",
    "\n",
    "model_a = df[[\"tax_id\", \"score\"]]\n",
    "model_a.to_parquet(\"data_source_model_a.parquet\")\n",
    "\n",
    "\n",
    "data_source_b = df[[\"tax_id\"]]\n",
    "data_source_b[\"valid\"] = np.random.choice(\n",
    "    [True, False], size=data_source_b.shape[0], p=[0.8, 0.2]\n",
    ")\n",
    "data_source_b.to_parquet(\"data_source_b.parquet\")\n",
    "\n",
    "model_b = df[[\"tax_id\", \"score\"]]\n",
    "model_b[\"score\"] = 1000 - model_b[\"score\"]\n",
    "model_b.to_parquet(\"data_source_model_b.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define the Policy\n",
    "\n",
    "This is all the code used to define the policy. \\\n",
    "In fact, in `docs/examples/policies/local/test_policy/policy.py` \n",
    "we use the exact same code to create our \"packaged\" version, \n",
    "which we'll use later for remote execution.\n",
    "\n",
    "There are a few key parts here:\n",
    "- `DataInputNode`: These nodes are used to bring data into your decision flows. Here, we'll use a local file, but this can later be replaced with an API or database without having to change the flow\n",
    "- `branch_condition` and `BranchNode`: \"Branches\" are how we make decisions in our policies. At a branch, you can have any number of possible outputs. In our case here, we write a function that returns \"approved\" if the score is greater than a cutoff.\n",
    "- `TerminateNode`: Terminate nodes are how we represent the final step in a policy, or the final decision. The `return_status` value is the final decision made. Here, we either approve or deny someone. We'll later see how this can be used to pass information to other systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from vulkan_public.spec.dependency import INPUT_NODE, Dependency\n",
    "from vulkan_public.spec.nodes import (\n",
    "    BranchNode,\n",
    "    DataInputNode,\n",
    "    TerminateNode,\n",
    "    TransformNode,\n",
    ")\n",
    "from vulkan_public.spec.policy import PolicyDefinition\n",
    "\n",
    "CONTINUE = \"CONTINUE\"\n",
    "\n",
    "\n",
    "class Status(Enum):\n",
    "    DENY_NO_DATA_A = \"DENY_NO_DATA_A\"\n",
    "    DENY_SCORE_A = \"DENY_SCORE_A\"\n",
    "    APPROVE = \"APPROVE\"\n",
    "\n",
    "\n",
    "# START: Data Source + Model A\n",
    "data_source_a = DataInputNode(\n",
    "    name=\"data_source_a\",\n",
    "    description=\"My first Data Source\",\n",
    "    source=DATA_SOURCE_A_ID,\n",
    "    dependencies={\"inputs\": Dependency(INPUT_NODE)},\n",
    ")\n",
    "\n",
    "check_source_a = BranchNode(\n",
    "    name=\"check_source_a\",\n",
    "    func=lambda source_a: CONTINUE\n",
    "    if source_a[\"valid\"]\n",
    "    else Status.DENY_NO_DATA_A.value,\n",
    "    dependencies={\"source_a\": Dependency(data_source_a.name)},\n",
    "    outputs=[CONTINUE, Status.DENY_NO_DATA_A.value],\n",
    ")\n",
    "\n",
    "deny_no_data_a = TerminateNode(\n",
    "    name=Status.DENY_NO_DATA_A.value,\n",
    "    return_status=Status.DENY_NO_DATA_A,\n",
    "    dependencies={\n",
    "        \"condition\": Dependency(check_source_a.name, Status.DENY_NO_DATA_A.value)\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "preprocess_model_a_request = TransformNode(\n",
    "    name=\"preprocess_model_a_request\",\n",
    "    description=\"Data Sources currently need to receive a structure that contains all the keys in the root level\",\n",
    "    func=lambda inputs, source_a, condition: {**inputs, **source_a},\n",
    "    dependencies={\n",
    "        \"inputs\": Dependency(INPUT_NODE),\n",
    "        \"source_a\": Dependency(data_source_a.name),\n",
    "        \"condition\": Dependency(check_source_a.name, CONTINUE),\n",
    "    },\n",
    ")\n",
    "\n",
    "model_a = DataInputNode(\n",
    "    name=\"Model A\",\n",
    "    description=\"Get score from Model A\",\n",
    "    source=MODEL_A_ID,\n",
    "    dependencies={\"data\": Dependency(preprocess_model_a_request.name)},\n",
    ")\n",
    "\n",
    "\n",
    "def _decide_model_a(context, response_model_a) -> str:\n",
    "    if response_model_a is None:\n",
    "        return Status.DENY_SCORE_A.value\n",
    "\n",
    "    elif response_model_a[\"score\"] >= context.env.get(\"THRESHOLD_MODEL_A\"):\n",
    "        return Status.DENY_SCORE_A.value\n",
    "\n",
    "    return Status.APPROVE.value\n",
    "\n",
    "\n",
    "decide_model_a = BranchNode(\n",
    "    name=\"decide_model_a\",\n",
    "    func=_decide_model_a,\n",
    "    dependencies={\"response_model_a\": Dependency(model_a.name)},\n",
    "    outputs=[Status.APPROVE.value, Status.DENY_SCORE_A.value],\n",
    ")\n",
    "\n",
    "deny_score_a = TerminateNode(\n",
    "    name=Status.DENY_SCORE_A.value,\n",
    "    return_status=Status.DENY_SCORE_A,\n",
    "    dependencies={\n",
    "        \"condition\": Dependency(decide_model_a.name, Status.DENY_SCORE_A.value)\n",
    "    },\n",
    ")\n",
    "# END Model with Source A\n",
    "\n",
    "approve = TerminateNode(\n",
    "    name=Status.APPROVE.value,\n",
    "    return_status=Status.APPROVE,\n",
    "    dependencies={\"condition\": Dependency(decide_model_a.name, Status.APPROVE.value)},\n",
    ")\n",
    "\n",
    "demo_policy = PolicyDefinition(\n",
    "    nodes=[\n",
    "        data_source_a,\n",
    "        check_source_a,\n",
    "        deny_no_data_a,\n",
    "        preprocess_model_a_request,\n",
    "        model_a,\n",
    "        decide_model_a,\n",
    "        deny_score_a,\n",
    "        approve,\n",
    "    ],\n",
    "    components=[],\n",
    "    config_variables=[\"THRESHOLD_MODEL_A\"],\n",
    "    input_schema={\"tax_id\": str},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy.from_definition(demo_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Run the Policy Locally \n",
    "\n",
    "The entire policy can be visualized and the run locally.\n",
    "\n",
    "To do that, we'll just have to do two things:\n",
    "1. Create a schema, telling Vulkan where to get data for the data sources;\n",
    "2. Set a value for our score \"cutoff\": the minimum score to be Approved;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Visualizing the flow of information\n",
    "\n",
    "We can visualize our policy locally, at all times. \\\n",
    "This can show us how the clients are being treated and where we're making each decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = PolicyRunner(policy, staging_path=\"./output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Running\n",
    "\n",
    "Now we're ready to run our policy.\n",
    "\n",
    "Let's start with a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_variables = {\"THRESHOLD_MODEL_A\": 650}\n",
    "\n",
    "result = runner.run(\n",
    "    input_data={\"tax_id\": \"1\"},\n",
    "    data_sources=data_sources,\n",
    "    config_variables=config_variables,\n",
    ")\n",
    "\n",
    "print(\"Here are our results:\\n\")\n",
    "result.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Running for a bunch of data\n",
    "\n",
    "We can run for 1 example, or for a bunch, just as easily. \\\n",
    "To run for a batch of data, we just need to pass the input data with a file. \\\n",
    "Let's pass in the input file we created at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_results = runner.run_batch(\n",
    "    input_data_path=\"input.parquet\",\n",
    "    data_sources=data_sources,\n",
    "    config_variables=config_variables,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Ramping it up: Two sources, two models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Preparing our new data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_b_test_schema = {\n",
    "    \"name\": DATA_SOURCE_B_ID,\n",
    "    \"keys\": [\"tax_id\"],\n",
    "    \"source\": {\n",
    "        \"path\": \"data_source_b.parquet\",\n",
    "    },\n",
    "}\n",
    "\n",
    "model_b_test_schema = {\n",
    "    \"name\": MODEL_B_ID,\n",
    "    \"keys\": [\"tax_id\"],\n",
    "    \"source\": {\n",
    "        \"path\": \"data_source_model_b.parquet\",\n",
    "    },\n",
    "}\n",
    "\n",
    "data_sources = [\n",
    "    DataSourceSpec.model_validate(data_source_a_test_schema),\n",
    "    DataSourceSpec.model_validate(model_a_test_schema),\n",
    "    DataSourceSpec.model_validate(data_source_b_test_schema),\n",
    "    DataSourceSpec.model_validate(model_b_test_schema),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from vulkan_public.spec.dependency import INPUT_NODE, Dependency\n",
    "from vulkan_public.spec.nodes import (\n",
    "    BranchNode,\n",
    "    DataInputNode,\n",
    "    TerminateNode,\n",
    "    TransformNode,\n",
    ")\n",
    "from vulkan_public.spec.policy import PolicyDefinition\n",
    "\n",
    "CONTINUE = \"CONTINUE\"\n",
    "\n",
    "\n",
    "class Status(Enum):\n",
    "    DENY_NO_DATA_A = \"DENY_NO_DATA_A\"\n",
    "    DENY_SCORE_A = \"DENY_SCORE_A\"\n",
    "    DENY_NO_DATA_B = \"DENY_NO_DATA_B\"\n",
    "    DENY_SCORE_B = \"DENY_SCORE_B\"\n",
    "    APPROVE = \"APPROVE\"\n",
    "\n",
    "\n",
    "# START: Data Source + Model A\n",
    "data_source_a = DataInputNode(\n",
    "    name=\"data_source_a\",\n",
    "    description=\"My first Data Source\",\n",
    "    source=DATA_SOURCE_A_ID,\n",
    "    dependencies={\"inputs\": Dependency(INPUT_NODE)},\n",
    ")\n",
    "\n",
    "\n",
    "def check_source_a(source_a) -> str:\n",
    "    if not source_a[\"valid\"]:\n",
    "        return Status.DENY_NO_DATA_A.value\n",
    "    return CONTINUE\n",
    "\n",
    "\n",
    "check_source_a = BranchNode(\n",
    "    name=\"check_source_a\",\n",
    "    func=_check_source_a,\n",
    "    dependencies={\"source_a\": Dependency(data_source_a.name)},\n",
    "    outputs=[CONTINUE, Status.DENY_NO_DATA_A.value],\n",
    ")\n",
    "\n",
    "deny_no_data_a = TerminateNode(\n",
    "    name=Status.DENY_NO_DATA_A.value,\n",
    "    return_status=Status.DENY_NO_DATA_A,\n",
    "    dependencies={\n",
    "        \"condition\": Dependency(check_source_a.name, Status.DENY_NO_DATA_A.value)\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "preprocess_model_a_request = TransformNode(\n",
    "    name=\"preprocess_model_a_request\",\n",
    "    description=\"Data Sources currently need to receive a structure that contains all the keys in the root level\",\n",
    "    func=lambda inputs, source_a, condition: {**inputs, **source_a},\n",
    "    dependencies={\n",
    "        \"inputs\": Dependency(INPUT_NODE),\n",
    "        \"source_a\": Dependency(data_source_a.name),\n",
    "        \"condition\": Dependency(check_source_a.name, CONTINUE),\n",
    "    },\n",
    ")\n",
    "\n",
    "model_a = DataInputNode(\n",
    "    name=\"Model A\",\n",
    "    description=\"Get score from Model A\",\n",
    "    source=MODEL_A_ID,\n",
    "    dependencies={\"data\": Dependency(preprocess_model_a_request.name)},\n",
    ")\n",
    "\n",
    "\n",
    "def _decide_model_a(context, response_model_a) -> str:\n",
    "    if response_model_a is None:\n",
    "        return Status.DENY_SCORE_A.value\n",
    "\n",
    "    elif response_model_a[\"score\"] >= context.env.get(\"THRESHOLD_MODEL_A\"):\n",
    "        return Status.DENY_SCORE_A.value\n",
    "\n",
    "    return CONTINUE\n",
    "\n",
    "\n",
    "decide_model_a = BranchNode(\n",
    "    name=\"decide_model_a\",\n",
    "    func=_decide_model_a,\n",
    "    dependencies={\"response_model_a\": Dependency(model_a.name)},\n",
    "    outputs=[CONTINUE, Status.DENY_SCORE_A.value],\n",
    ")\n",
    "\n",
    "\n",
    "deny_score_a = TerminateNode(\n",
    "    name=Status.DENY_SCORE_A.value,\n",
    "    return_status=Status.DENY_SCORE_A,\n",
    "    dependencies={\n",
    "        \"condition\": Dependency(decide_model_a.name, Status.DENY_SCORE_A.value)\n",
    "    },\n",
    ")\n",
    "# END Model with Source A\n",
    "\n",
    "# Start Data Source and Model B\n",
    "make_source_b_request = TransformNode(\n",
    "    name=\"make_source_b_request\",\n",
    "    func=lambda inputs, condition: inputs,\n",
    "    dependencies={\n",
    "        \"inputs\": Dependency(INPUT_NODE),\n",
    "        \"condition\": Dependency(decide_model_a.name, CONTINUE),\n",
    "    },\n",
    ")\n",
    "\n",
    "data_source_b = DataInputNode(\n",
    "    name=\"data_source_b\",\n",
    "    description=\"The Second Data Source!\",\n",
    "    source=DATA_SOURCE_B_ID,\n",
    "    dependencies={\"data\": Dependency(make_source_b_request.name)},\n",
    ")\n",
    "\n",
    "check_source_b = BranchNode(\n",
    "    name=\"check_source_b\",\n",
    "    func=lambda data: CONTINUE if data[\"valid\"] else Status.DENY_NO_DATA_B.value,\n",
    "    dependencies={\"data\": Dependency(data_source_b.name)},\n",
    "    outputs=[CONTINUE, Status.DENY_NO_DATA_B.value],\n",
    ")\n",
    "\n",
    "deny_no_data_b = TerminateNode(\n",
    "    name=Status.DENY_NO_DATA_B.value,\n",
    "    return_status=Status.DENY_NO_DATA_B,\n",
    "    dependencies={\n",
    "        \"condition\": Dependency(check_source_b.name, Status.DENY_NO_DATA_B.value)\n",
    "    },\n",
    ")\n",
    "\n",
    "preprocess_model_b_request = TransformNode(\n",
    "    name=\"preprocess_model_b_request\",\n",
    "    description=\"Data Sources currently need to receive a structure that contains all the keys in the root level\",\n",
    "    func=lambda inputs, source_a, source_b, condition: {\n",
    "        **inputs,\n",
    "        **source_a,\n",
    "        **source_b,\n",
    "    },\n",
    "    dependencies={\n",
    "        \"inputs\": Dependency(INPUT_NODE),\n",
    "        \"source_a\": Dependency(data_source_a.name),\n",
    "        \"source_b\": Dependency(data_source_b.name),\n",
    "        \"condition\": Dependency(check_source_b.name, CONTINUE),\n",
    "    },\n",
    ")\n",
    "\n",
    "model_b = DataInputNode(\n",
    "    name=\"model_b\",\n",
    "    description=\"Get score from Model B using two data sources\",\n",
    "    source=MODEL_B_ID,\n",
    "    dependencies={\"data\": Dependency(preprocess_model_b_request.name)},\n",
    ")\n",
    "\n",
    "\n",
    "def _decide_model_b(context, model_response) -> str:\n",
    "    if model_response is None:\n",
    "        return Status.DENY_SCORE_B.value\n",
    "\n",
    "    elif model_response[\"score\"] >= context.env.get(\"THRESHOLD_MODEL_B\"):\n",
    "        return Status.DENY_SCORE_B.value\n",
    "\n",
    "    return Status.APPROVE.value\n",
    "\n",
    "\n",
    "decide_model_b = BranchNode(\n",
    "    name=\"decide_model_b\",\n",
    "    func=_decide_model_b,\n",
    "    dependencies={\"model_response\": Dependency(model_b.name)},\n",
    "    outputs=[Status.APPROVE.value, Status.DENY_SCORE_B.value],\n",
    ")\n",
    "\n",
    "\n",
    "deny_score_b = TerminateNode(\n",
    "    name=Status.DENY_SCORE_B.value,\n",
    "    return_status=Status.DENY_SCORE_B,\n",
    "    dependencies={\n",
    "        \"condition\": Dependency(decide_model_b.name, Status.DENY_SCORE_B.value)\n",
    "    },\n",
    ")\n",
    "# END Model with Sources A and B\n",
    "\n",
    "approve = TerminateNode(\n",
    "    name=Status.APPROVE.value,\n",
    "    return_status=Status.APPROVE,\n",
    "    dependencies={\"condition\": Dependency(decide_model_b.name, Status.APPROVE.value)},\n",
    ")\n",
    "\n",
    "demo_policy = PolicyDefinition(\n",
    "    nodes=[\n",
    "        data_source_a,\n",
    "        check_source_a,\n",
    "        deny_no_data_a,\n",
    "        preprocess_model_a_request,\n",
    "        model_a,\n",
    "        decide_model_a,\n",
    "        deny_score_a,\n",
    "        make_source_b_request,\n",
    "        data_source_b,\n",
    "        check_source_b,\n",
    "        deny_no_data_b,\n",
    "        preprocess_model_b_request,\n",
    "        model_b,\n",
    "        decide_model_b,\n",
    "        deny_score_b,\n",
    "        approve,\n",
    "    ],\n",
    "    components=[],\n",
    "    config_variables=[\"THRESHOLD_MODEL_A\", \"THRESHOLD_MODEL_B\"],\n",
    "    input_schema={\"tax_id\": str},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy.from_definition(demo_policy)\n",
    "runner = PolicyRunner(policy, staging_path=\"./output/\")\n",
    "runner.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_results = runner.run_batch(\n",
    "    input_data_path=\"input.parquet\",\n",
    "    data_sources=data_sources,\n",
    "    config_variables={\"THRESHOLD_MODEL_A\": 650, \"THRESHOLD_MODEL_B\": 700},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results.data.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
