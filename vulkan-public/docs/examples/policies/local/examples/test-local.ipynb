{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a root logger so that other logs are piped to the notebook.\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from IPython.display import SVG, Image, display\n",
    "from vulkan.core.policy import Policy\n",
    "\n",
    "from vulkan_public.spec.dependency import INPUT_NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import signal\n",
    "import socket\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def is_server_running(host: str, port: int, timeout: float = 1.0) -> bool:\n",
    "    \"\"\"Check if a server is running on the given host and port.\n",
    "\n",
    "    Args:\n",
    "        host (str): The hostname or IP address of the server.\n",
    "        port (int): The port number to check.\n",
    "        timeout (float): The timeout for the connection in seconds (default is 1.0).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a server is running, False otherwise.\n",
    "    \"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.settimeout(timeout)  # Set timeout for the connection attempt\n",
    "        try:\n",
    "            sock.connect((host, port))\n",
    "            return True\n",
    "        except (socket.timeout, ConnectionRefusedError, OSError):\n",
    "            return False\n",
    "\n",
    "\n",
    "def render_pipeline(\n",
    "    pipeline_builder,\n",
    "    output_image_path: str,\n",
    "    render_port: int = -1,\n",
    "):\n",
    "    if render_port > 0:\n",
    "        busy = is_server_running(\"localhost\", render_port, timeout=1)\n",
    "        if busy:\n",
    "            msg = (\n",
    "                f\"There is already a server running on port {render_port}. \\n\"\n",
    "                \"Skipping starting a server for this render to avoid conflict.\"\n",
    "            )\n",
    "            print(msg)\n",
    "            render_port = -1\n",
    "\n",
    "    args = [\n",
    "        \"--runner=apache_beam.runners.render.RenderRunner\",\n",
    "        f\"--render_output={output_image_path}\",\n",
    "        f\"--render_port={render_port}\",\n",
    "    ]\n",
    "    options = PipelineOptions(args)\n",
    "\n",
    "    p = pipeline_builder.build_single_run_pipeline(\n",
    "        input_data={}, pipeline_options=options\n",
    "    )\n",
    "    p.run()\n",
    "    return SVG(output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"simple_bkt_lg.csv\").iloc[:5]\n",
    "df[\"month\"] = df[\"month\"].astype(str)\n",
    "df[\"tax_id\"] = df[\"tax_id\"].astype(str)\n",
    "\n",
    "df.to_parquet(\"input.parquet\")\n",
    "\n",
    "lookup_df = df[[\"tax_id\"]]\n",
    "lookup_df[\"squared\"] = lookup_df[\"tax_id\"].astype(int) ** 2\n",
    "lookup_df.to_parquet(\"file_data_source.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from vulkan_public.spec.dependency import INPUT_NODE, Dependency\n",
    "from vulkan_public.spec.nodes import BranchNode, DataInputNode, TerminateNode\n",
    "from vulkan_public.spec.policy import PolicyDefinition\n",
    "\n",
    "\n",
    "class Status(Enum):\n",
    "    APPROVED = \"APPROVED\"\n",
    "    DENIED = \"DENIED\"\n",
    "\n",
    "\n",
    "sample_api = DataInputNode(\n",
    "    name=\"sample_api\",\n",
    "    description=\"DataInputNode data\",\n",
    "    source=\"vendor-name:api-name:v0.0.1\",\n",
    "    dependencies={\"inputs\": Dependency(INPUT_NODE)},\n",
    ")\n",
    "\n",
    "sample_file_input = DataInputNode(\n",
    "    name=\"sample_file_input\",\n",
    "    description=\"DataInputNode with File Input\",\n",
    "    source=\"file-input:api-name:v0.0.2\",\n",
    "    dependencies={\"inputs\": Dependency(INPUT_NODE)},\n",
    ")\n",
    "\n",
    "\n",
    "# Branching node\n",
    "def branch_condition_1(context, scores, file_inputs, **kwargs):\n",
    "    context.log.info(f\"BranchNode data: {scores}\")\n",
    "    context.log.info(f\"File Input data: {file_inputs}\")\n",
    "    context.log.info(f\"File Input data: {type(file_inputs)}\")\n",
    "    if file_inputs[\"squared\"] > context.env.get(\"SCORE_CUTOFF\", 500):\n",
    "        return \"approved\"\n",
    "    return \"denied\"\n",
    "\n",
    "\n",
    "branch_1 = BranchNode(\n",
    "    func=branch_condition_1,\n",
    "    name=\"branch_1\",\n",
    "    description=\"BranchNode data\",\n",
    "    dependencies={\n",
    "        \"scores\": Dependency(sample_api.name),\n",
    "        \"file_inputs\": Dependency(sample_file_input.name),\n",
    "    },\n",
    "    outputs=[\"approved\", \"denied\"],\n",
    ")\n",
    "\n",
    "\n",
    "approved = TerminateNode(\n",
    "    name=\"approved\",\n",
    "    description=\"TerminateNode data branch\",\n",
    "    return_status=Status.APPROVED,\n",
    "    dependencies={\"condition\": Dependency(\"branch_1\", \"approved\")},\n",
    ")\n",
    "\n",
    "\n",
    "denied = TerminateNode(\n",
    "    name=\"denied\",\n",
    "    description=\"TerminateNode data branch\",\n",
    "    return_status=Status.DENIED,\n",
    "    dependencies={\"condition\": Dependency(\"branch_1\", \"denied\")},\n",
    ")\n",
    "\n",
    "\n",
    "demo_policy = PolicyDefinition(\n",
    "    nodes=[\n",
    "        sample_api,\n",
    "        sample_file_input,\n",
    "        branch_1,\n",
    "        approved,\n",
    "        denied,\n",
    "    ],\n",
    "    components=[],\n",
    "    config_variables=[\"SCORE_CUTOFF\"],\n",
    "    input_schema={\"tax_id\": str, \"score\": int},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy.from_definition(demo_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Run the Policy Locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vulkan.beam.local.runner import PolicyRunner\n",
    "\n",
    "from vulkan_public.schemas import DataSourceSpec\n",
    "\n",
    "test_url_schema = {\n",
    "    \"name\": \"vendor-name:api-name:v0.0.1\",\n",
    "    \"keys\": [\"tax_id\"],\n",
    "    \"source\": {\n",
    "        \"url\": \"http://localhost:5000/serasa\",\n",
    "    },\n",
    "    \"caching\": {\n",
    "        \"enabled\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "# TODO: file inputs should be equivalent locally and remote so we can run on DataFlow\n",
    "test_file_schema = {\n",
    "    \"name\": \"file-input:api-name:v0.0.2\",\n",
    "    \"keys\": [\"tax_id\"],\n",
    "    \"source\": {\n",
    "        # \"id\": \"...\"\n",
    "        \"path\": \"file_data_source.parquet\",\n",
    "    },\n",
    "    \"caching\": {\n",
    "        \"enabled\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "data_sources = [\n",
    "    # DataSourceSpec.model_validate(test_url_schema),\n",
    "    DataSourceSpec.model_validate(test_file_schema),\n",
    "]\n",
    "\n",
    "# TODO: The current implementation assumes a single \"backfill\" job, ie.\n",
    "#       a single set of parameters.\n",
    "#       We still need to support the \"backtest\" interface for testing multiple\n",
    "#       configurations for the same pipeline.\n",
    "config_variables = {\"SCORE_CUTOFF\": 500}\n",
    "\n",
    "runner = PolicyRunner(policy, staging_path=\"./output/\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10",
   "metadata": {},
   "source": [
    "output_image_path = \"dag.svg\"\n",
    "\n",
    "render_pipeline(builder, output_image_path, render_port=21111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = runner.run(\n",
    "    input_data={\"tax_id\": \"0\", \"score\": 100},\n",
    "    data_sources=data_sources,\n",
    "    config_variables=config_variables,\n",
    ")\n",
    "\n",
    "result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = runner.run_batch(\n",
    "    input_data_path=\"input.parquet\",\n",
    "    data_sources=data_sources,\n",
    "    config_variables=config_variables,\n",
    ")\n",
    "batch_results.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "1. Suite de desenvolvimento atrativa o suficiente\n",
    "    - Fácil de escrever\n",
    "    - Fácil de ler\n",
    "    - Fácil de testar e iterar\n",
    "    - Fácil de levar pra produção\n",
    "        - Em uma solução self-service, conseguir colocar em produção em 1 linha\n",
    "        - Em uma empresa menor, precisa conseguir passar pra equipe que roda\n",
    "        - Em uma empresa que já tem uma solução, precisa ser fácil integrar com o que já existe\n",
    "            - Fácil explicar\n",
    "            - Talvez gerar um fluxograma pra implementação?\n",
    "            - Integrar via API com soluções existentes\n",
    "2. Produto usado em Desenvolvimento\n",
    "    1. Qual o trigger pra testar?\n",
    "3. Vantagens de usar a aplicação?\n",
    "    1. Na etapa de análise\n",
    "        - Escalabilidade -> Plataforma de desenvolvimento: Jupyter-like? Databricks?\n",
    "        - Rodar múltiplos testes / backtests simultaneamente\n",
    "    2. Na etapa de servir\n",
    "        - Automação\n",
    "        - Experimentação fácil\n",
    "4. Produto usado em Execução e Monitoramento "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Running remotely in Vulkan Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vulkan_public.cli import client as vulkan\n",
    "from vulkan_public.cli.context import Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = Context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_source_id = vulkan.data.create_data_source(\n",
    "    ctx,\n",
    "    config=test_url_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info = vulkan.backtest.upload_backtest_file(\n",
    "    ctx,\n",
    "    file_name=\"Data Source Reference\",\n",
    "    file_path=\"file_data_source.parquet\",\n",
    "    file_format=\"PARQUET\",\n",
    "    schema={\"tax_id\": \"str\", \"squared\": \"int\"},\n",
    ")\n",
    "file_id = file_info[\"uploaded_file_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_file_source_schema = {\n",
    "    \"name\": \"file-input:api-name:v0.0.2\",\n",
    "    \"keys\": [\"tax_id\"],\n",
    "    \"source\": {\"file_id\": file_id},\n",
    "    \"caching\": {\n",
    "        \"enabled\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "file_data_source_id = vulkan.data.create_data_source(\n",
    "    ctx,\n",
    "    config=remote_file_source_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_path = os.path.abspath(\"..\")\n",
    "\n",
    "policy_id = vulkan.policy.create_policy(ctx, \"Test Beam Policy\")\n",
    "policy_version_id = vulkan.policy.create_policy_version(\n",
    "    ctx, policy_id=policy_id, version_name=\"v0\", repository_path=policy_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulkan.policy_version.create_backtest_workspace(ctx, policy_version_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_info = vulkan.backtest.create_backtest(\n",
    "    ctx,\n",
    "    policy_version_id=policy_version_id,\n",
    "    input_file_id=file_id,\n",
    "    config_variables=[\n",
    "        {\"SCORE_CUTOFF\": 500},\n",
    "        # {\"SCORE_CUTOFF\": 700},\n",
    "    ],\n",
    "    metrics_config={\n",
    "        \"target_column\": \"squared\",\n",
    "    },\n",
    ")\n",
    "\n",
    "backtest_id = backtest_info[\"backtest_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulkan.backtest.poll_backtest_status(ctx, backtest_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vulkan.backtest.get_results(ctx, backtest_id)\n",
    "output_data = pd.DataFrame(output)\n",
    "output_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
